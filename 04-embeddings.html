<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Embeddings - Large Language Models for Software Engineers</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Large Language Models for Software Engineers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="embeddings"><a class="header" href="#embeddings">Embeddings</a></h1>
<h2 id="what-are-embeddings"><a class="header" href="#what-are-embeddings">What are Embeddings?</a></h2>
<p>Embeddings are a way to represent text as a semantically meaningful vector of numbers.
The core idea is that if two texts are similar, then their vector representations should be similar as well.</p>
<p>For example, the embeddings of "I love programming in Python" and "I like coding in a language whose symbol is a snake" should be similar despite the fact that the texts have practically no words in common.
This is called <strong>semantic similarity</strong> as opposed to syntactic similarity which is about the simple similarity of the sentence structure and the words used.</p>
<p>Depending on the use case, you can embed words, sentences, paragraphs, or even entire documents.</p>
<p>The concept of embeddings—and their similarities—is useful for many applications:</p>
<ul>
<li><strong>Semantic search</strong>: You can use embeddings to find the most similar texts to a given query</li>
<li><strong>Clustering</strong>: You can use embeddings to cluster texts into different groups based on their semantic similarity</li>
<li><strong>Recommendation systems</strong>: You can use embeddings to recommend similar items to a given item</li>
</ul>
<p>In later chapters, we'll also explore how to use embeddings to build RAG pipelines that enhance the quality of your LLM applications.</p>
<p>So how are embeddings generated?
Interestingly, large language models (LLMs) can produce them as a byproduct of their architecture.</p>
<p>After the tokenizer has converted the text into tokens, a so-called embedding layer transforms every token into a high-dimensional vector.
These vectors are continuously refined through the transformer layers until an "unembedding layer" produces the final output—the logits over the vocabulary.
Since an LLM is trained to predict the next token, its embedding layer automatically learns to represent tokens in a semantically meaningful way.</p>
<p>Alternatively, you can use specialized embedding models trained specifically to produce high-quality embeddings.</p>
<p>OpenAI provides a range of embedding models, the most important of which are the <code>text-embedding-3-small</code> and <code>text-embedding-3-large</code> models.
You can use them like this:</p>
<pre><code class="language-python">import os, requests

response = requests.post(
    "https://api.openai.com/v1/embeddings",
    headers={
        "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
        "Content-Type": "application/json",
    },
    json={
        "input": "Your text string goes here",
        "model": "text-embedding-3-small"
    }
)

response_json = response.json()
embedding = response_json["data"][0]["embedding"]
print(embedding[:5])
print(len(embedding))
</code></pre>
<p>This will output something along the lines of:</p>
<pre><code>[0.005132983, 0.017242905, -0.018698474, -0.018558515, -0.047250036]
1536
</code></pre>
<p>Note that embeddings are typically high-dimensional.
For example, the <code>text-embedding-3-small</code> model produces 1536-dimensional embeddings while the <code>text-embedding-3-large</code> model produces 3072-dimensional embeddings.</p>
<p>In general, higher-dimensional embeddings capture more nuanced relationships but can be slower to compute and more memory-intensive to store.</p>
<h2 id="embedding-similarity"><a class="header" href="#embedding-similarity">Embedding Similarity</a></h2>
<p>Remember, the core idea behind embeddings is that semantically similar texts should have similar vector representations.
But how can we actually calculate the similarity between two embeddings?</p>
<p>Embeddings are vectors, and vector similarity is commonly measured using <strong>cosine similarity</strong>, defined as:</p>
<p>$$
\text{similarity}(\vec{v}, \vec{w}) = \cos(\theta) = \frac{\vec{v} \cdot \vec{w}}{|\vec{v}| |\vec{w}|}
$$</p>
<p>where \(\theta\) is the angle between the vectors \(\vec{v}\) and \(\vec{w}\), \(\vec{v} \cdot \vec{w}\) is the dot product of the vectors and \(|\vec{v}|\) and \(|\vec{w}|\) are their norms.</p>
<p><img src="images/cosine_similarity.png" alt="Cosine Similarity" /></p>
<p>As a reminder, the dot product (also called the inner product) of two vectors is defined as:</p>
<p>$$
\vec{v} \cdot \vec{w} = \sum_{i=1}^{n} v_i w_i
$$</p>
<blockquote>
<p>If we get very technical, the dot product is a concrete example of an inner product which is a more general concept.
In the context of embeddings, however, these two terms are used interchangeably.</p>
</blockquote>
<p>The norm of a vector is defined as:</p>
<p>$$
|\vec{v}| = \sqrt{\sum_{i=1}^{n} v_i^2}
$$</p>
<p>The cosine similarity is:</p>
<ul>
<li>equal to 1 if the vectors have the same direction,</li>
<li>equal to 0 if the vectors are orthogonal,</li>
<li>equal to -1 if the vectors have opposite directions.</li>
</ul>
<p>Generally speaking, the closer the cosine similarity is to 1, the more similar the vectors are.
The closer it is to -1, the more dissimilar they are.</p>
<p>Here is how we could implement the cosine similarity in Python:</p>
<pre><code class="language-python">def get_norm(v):
  return math.sqrt(sum(x ** 2 for x in v))

def get_dot_product(v, w):
  return sum(v[i] * w[i] for i in range(len(v)))

def get_cosine_similarity(v, w):
  return get_dot_product(v, w) / (get_norm(v) * get_norm(w))

v = [1, 0]
w = [1, 1]

print(get_norm(v)) # 1.0
print(get_norm(w)) # 1.41...
print(get_dot_product(v, w)) # 1
print(get_cosine_similarity(v, w)) # 0.707...
</code></pre>
<p>Note that you typically shouldn't use plain Python implementations for mathematical operations like norms or dot products.
Instead, rely on libraries like NumPy or SciPy because the latter will <strong>vectorize</strong> the operations which is much more efficient than using regular Python loops.</p>
<p>Developers often use the dot product—or even Euclidean distance—to measure similarity instead of the cosine similarity.
This works because embeddings are usually normalized to unit length, i.e. they have a norm of 1.</p>
<p>Let's verify that this is true for the embeddings produced by OpenAI:</p>
<pre><code class="language-python">import math

def get_norm(embedding):
    return math.sqrt(sum(x ** 2 for x in embedding))

# Here embedding is some embedding from OpenAI
# (for example, you can use the embedding from the previous section)
print(get_norm(embedding)) # 1.0
</code></pre>
<p>If two embeddings are normalized to unit length—that is, their norms are 1—their cosine similarity is equal to their dot product:</p>
<p>$$
\cos(\theta) = \frac{\vec{v} \cdot \vec{w}}{|\vec{v}| |\vec{w}|} = \vec{v} \cdot \vec{w}
$$</p>
<p>Similarly, the Euclidean distance of two unit-length vectors becomes a monotonic transformation of the cosine similarity:</p>
<p>$$
|\vec{v} - \vec{w}|^2 = |\vec{v}|^2 + |\vec{w}|^2 - 2 \vec{v} \cdot \vec{w} = 2 - 2 \cos(\theta)
$$</p>
<p>Therefore:</p>
<p>$$
|\vec{v} - \vec{w}| = \sqrt{2 - 2 \cos(\theta)}
$$</p>
<p>This means that for unit-length embeddings, ranking by cosine similarity is equivalent to ranking by dot product or Euclidean distance.
However, this equivalence holds only for unit-length vectors.</p>
<p>Therefore, when using similarities other than the cosine similarity, you should always verify that the embeddings produced by the embedding model you are using are normalized to unit length.</p>
<h2 id="vector-databases"><a class="header" href="#vector-databases">Vector Databases</a></h2>
<p>Vector databases provide an efficient way to store and retrieve embeddings, with their primary purpose being to enable fast similarity searches.
When working with a large number of embeddings, we would theoretically have to compare a query embedding to all others to find the nearest neighbors.
This process becomes increasingly slow as the number of embeddings grows.
To address this, vector databases use specialized algorithms to accelerate the search process.</p>
<p>One of the most widely used algorithms for efficient similarity search is <strong>IVFFlat</strong> (short for InVerted File Flat).</p>
<p>The IVFFlat algorithm works by partitioning the embedding space into cells with centroids.</p>
<p><img src="images/ivfflat_cells.png" alt="IVFFlat Cells" /></p>
<p>At search time, the algorithm first finds the nearest centroids and then performs a search only inside those cells.</p>
<p><img src="images/ivfflat_search.png" alt="IVFFlat Search" /></p>
<p>In other words, the algorithm performs the following steps to find the best embeddings for a query embedding \(\vec{v}\):</p>
<ol>
<li>Calculate the distance between \(\vec{v}\) and all centroids.</li>
<li>Find the \(k\) centroids with the smallest distance to \(\vec{v}\).</li>
<li>Calculate the distance between \(\vec{v}\) and all embeddings within the cells corresponding to the \(k\) centroids from step 2.</li>
<li>Return the embeddings with the smallest distance to \(\vec{v}\).</li>
</ol>
<p>The cells and their centroids must be learned from the data in advance, which is why we typically build the index only after inserting some initial data.</p>
<p>It's important to note that, like most similarity search algorithms used in vector databases, IVFFlat performs only an <strong>approximate nearest neighbor search</strong>.
As a result, it may not always return the exact nearest neighbors, depending on the location of the query embedding in the vector space.
This trade-off prioritizes performance over absolute accuracy.</p>
<p>This also means that unlike traditional database indices, a bad IVFFlat index can reduce the search quality, so you need to carefully tune its parameters and build it on a representative dataset.</p>
<p>Although dedicated vector databases like Faiss exist, we will explore the <code>pgvector</code> extension for Postgres to store embeddings in this section.</p>
<p>First, start a local PostgreSQL database:</p>
<pre><code class="language-bash">docker run -d --name pgvector-db \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=Secret123!      \
  -e POSTGRES_DB=vectordb              \
  -p 5432:5432                         \
  -v pgdata:/var/lib/postgresql/data   \
  pgvector/pgvector:pg17
</code></pre>
<p>Connect to the database:</p>
<pre><code class="language-bash">docker exec -it pgvector-db psql -U postgres -d vectordb
</code></pre>
<p>Check whether the <code>vector</code> extension is enabled:</p>
<pre><code class="language-sql">SELECT extname, extversion
FROM   pg_extension
WHERE  extname = 'vector';
</code></pre>
<p>If the extension is not enabled, enable it:</p>
<pre><code class="language-sql">CREATE EXTENSION vector;
</code></pre>
<p>Now, create a table to store the embeddings:</p>
<pre><code class="language-sql">CREATE TABLE items (
  id        SERIAL PRIMARY KEY,
  content   TEXT,
  embedding VECTOR(3)
);
</code></pre>
<blockquote>
<p>In reality, the embedding dimension should be much larger: we only use 3 because this is a toy example.</p>
</blockquote>
<p>Insert some data into the table:</p>
<pre><code class="language-sql">INSERT INTO items (content, embedding) VALUES
  ('apple',  '[0.1,0.2,0.3]'),
  ('banana', '[0.11,0.19,0.29]'),
  ('car',    '[0.9,0.8,0.7]');
</code></pre>
<p>Double-check that the data was inserted correctly:</p>
<pre><code class="language-sql">SELECT * FROM items;
</code></pre>
<p>Now we can run our first similarity search:</p>
<pre><code class="language-sql">SELECT id, content, embedding &lt;-&gt; '[0.1,0.2,0.25]' AS dist
FROM items
ORDER BY dist
LIMIT 2;
</code></pre>
<p>This returns the embeddings for <code>banana</code> and <code>apple</code>, which are the two closest to <code>[0.1,0.2,0.25]</code>.</p>
<p>It's important to note that <code>pgvector</code> technically works with distances and not with similarities.
The difference is straightforward: the larger the distance, the smaller the similarity, and vice versa.
After all, two vectors with high similarity should be close together, while those with low similarity should be far apart.</p>
<p>In fact, it may be more intuitive to think in terms of distance rather than similarity. While the concept of "similarity" between vectors can be somewhat abstract, distance is a straightforward geometric measure that is immediately understandable.</p>
<p>The <code>pgvector</code> extension supports three operators for computing distance:</p>
<ul>
<li><code>&lt;-&gt;</code> for the Euclidean distance</li>
<li><code>&lt;#&gt;</code> for the negative inner product</li>
<li><code>&lt;=&gt;</code> for the cosine distance which is defined as <code>1 - cosine similarity</code></li>
</ul>
<p>Note that <code>&lt;#&gt;</code> is the negative inner product because <code>&lt;#&gt;</code> is supposed to be a distance operator.
Similarly, <code>&lt;=&gt;</code> represents the cosine distance, not the cosine similarity.</p>
<p>We can use the operators like this:</p>
<pre><code class="language-sql">SELECT
  '[0.1,0.2,0.3]'::vector &lt;-&gt; '[0, 0.1, 0.2]'::vector  AS euclidean_distance,
  '[0.1,0.2,0.3]'::vector &lt;#&gt; '[0, 0.1, 0.2]'::vector  AS neg_inner_product,
  '[0.1,0.2,0.3]'::vector &lt;=&gt; '[0, 0.1, 0.2]'::vector  AS cosine_distance;
</code></pre>
<p>This will output approximately:</p>
<ul>
<li><code>0.1732</code> for the Euclidean distance</li>
<li><code>-0.0800</code> for the negative inner product</li>
<li><code>0.0438</code> for the cosine distance</li>
</ul>
<p>We can verify our results in Python:</p>
<pre><code class="language-python">import math

def get_distance(v, w):
  return math.sqrt(sum((v[i] - w[i]) ** 2 for i in range(len(v))))

def get_dot_product(v, w):
  return sum(v[i] * w[i] for i in range(len(v)))

def get_norm(v):
  return math.sqrt(sum(x ** 2 for x in v))

def get_cosine_similarity(v, w):
  return get_dot_product(v, w) / (get_norm(v) * get_norm(w))

v = [0.1, 0.2, 0.3]
w = [0, 0.1, 0.2]

print("Euclidean distance:", get_distance(v, w))
print("Negative inner product:", -get_dot_product(v, w))
print("Cosine distance:", 1 - get_cosine_similarity(v, w))
</code></pre>
<p>This will output approximately:</p>
<ul>
<li><code>0.1732</code> for the Euclidean distance</li>
<li><code>-0.0800</code> for the negative inner product</li>
<li><code>0.0438</code> for the cosine distance</li>
</ul>
<p>These values match those returned by <code>pgvector</code>.</p>
<p>If all vector databases did was compute distances, implementing one would be relatively straightforward.
However, remember that their primary purpose is to support efficient distance-based search.</p>
<p>We won't see meaningful performance gains with just three items.
So, let's drop the current table, create a new one, and insert a million random 512-dimensional embeddings along with some dummy content.</p>
<pre><code class="language-sql">DROP TABLE items;

CREATE TABLE items (
  id        SERIAL PRIMARY KEY,
  content   TEXT,
  embedding VECTOR(512)
);

INSERT INTO items (content, embedding)
SELECT
  'rand-' || g,
  ARRAY(
    SELECT random()
    FROM generate_series(1, 512)
  )::vector(512)
FROM generate_series(1, 1000000) AS g;
</code></pre>
<blockquote>
<p>This command will take a while to complete.</p>
</blockquote>
<p>We should double check that the data was inserted correctly by looking at the first 10 rows and the total number of rows:</p>
<pre><code class="language-sql">SELECT * FROM items LIMIT 10;
SELECT COUNT(*) FROM items;
</code></pre>
<p>Let's perform a simple similarity search and find the 5 nearest neighbors of the vector <code>[1, 1, 1, ...]</code>:</p>
<pre><code class="language-sql">WITH q AS (
  SELECT array_fill(1::float8, ARRAY[512])::vector(512) AS v
)
SELECT id, content
FROM   items, q
ORDER  BY embedding &lt;=&gt; q.v
LIMIT  5;
</code></pre>
<p>This takes a few seconds to complete.
If we explain the query by prefixing it with <code>EXPLAIN ANALYZE</code>, we can see that the query is performing a sequential scan of the table:</p>
<pre><code>Sort Method: top-N heapsort [...]
    -&gt;  Nested Loop [...]
        -&gt;  CTE Scan on q [...]
        -&gt;  Seq Scan on items [...]
</code></pre>
<p>We can now add an IVFFlat index to the table.
When creating the index, we choose the number of <code>lists</code> which determines the number of cells to use.
Then, at query time, we choose <code>probes</code> which determines the number of nearest cells to consider.</p>
<p>The official documentation recommends settings <code>lists</code> to <code>rows / 1000</code> for up to 1M rows and <code>lists</code> for <code>sqrt(rows)</code> for over 1M rows.
Additionally, it recommends setting <code>probes</code> to <code>sqrt(lists)</code> as a good starting point.</p>
<p>Let's create the index with the recommended parameters:</p>
<pre><code class="language-sql">CREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 1000);
SET ivfflat.probes = 30;
</code></pre>
<p>This command will take a while because it has to build the index from scratch—the cells and their centroids have to be learned from the data.</p>
<p>Now, let's run a similarity search again:</p>
<pre><code class="language-sql">WITH q AS (
  SELECT array_fill(0.0::float8, ARRAY[512])::vector(512) AS v
)
SELECT id, content
FROM   items, q
ORDER  BY embedding &lt;=&gt; q.v
LIMIT  5;
</code></pre>
<p>This should result in a significant improvement in query performance compared to the sequential scan and this improvement will only become more pronounced as the number of embeddings grows.</p>
<p>If we explain the query by prefixing it with <code>EXPLAIN ANALYZE</code>, we can see that the query is now using the IVFFlat index:</p>
<pre><code>-&gt;  Index Scan using items_embedding_idx on items
</code></pre>
<p>You can drop the index again by running:</p>
<pre><code class="language-sql">DROP INDEX items_embedding_idx;
</code></pre>
<p>Try rebuilding the index with different values for <code>lists</code> and <code>probes</code> and see how the performance changes.</p>
<p>There are other indices that you can use for similarity search.
For example, <code>pgvector</code> also supports the HNSW (Hierarchical Navigable Small World) index.
Additionally, other vector databases like Faiss support even more sophisticated indices.</p>
<p>For most practical purposes, <code>pgvector</code> combined with the IVFFlat index is sufficient.
Nevertheless, we encourage you to explore other vector databases and indices to find the best fit for your use case.
After all, the core idea behind all vector databases is the same: they enable us to store embeddings and perform efficient similarity searches using specialized indices.</p>
<h2 id="hybrid-search-and-rank-fusion"><a class="header" href="#hybrid-search-and-rank-fusion">Hybrid Search and Rank Fusion</a></h2>
<p>The problem with a pure embedding search is that we are not guaranteed to find potentially important exact matches.</p>
<p>Let's consider the following document collection:</p>
<pre><code class="language-python">documents = [
    "TS-01 Can't access my account with my password",
    "TS-02 My password is not working and I don't know what it is so I need help",
    "TS-03 I need help with my account and I can't log in",
    "TS-04 I am having trouble with my setup and I don't know what it is",
    "TS-05 I can't access my account with my password",
    "TS-06 I need help",
]
documents = [doc.split() for doc in documents]
</code></pre>
<p>Take a customer support ticket containing an identifier such as "TS-01".
An embedding search might miss this exact match because embeddings are high-dimensional vectors whose results are difficult to interpret and do not guarantee the retrieval of critical terms.
Therefore, in such a case it would be useful to combine the results of a semantic search with those of a traditional keyword search.</p>
<p>Before we cover traditional keyword search, we will first need to introduce a concept called <strong>inverse document frequency</strong>, or IDF, which measures how specific a keyword is in a document collection.
The core idea behind IDF is that the specificity of a keyword is inversely proportional to the number of documents that contain it:</p>
<p>$$
\text{IDF}(q) = \log \left(\frac{N - n(q) + 0.5}{n(q) + 0.5} + 1\right)
$$</p>
<p>where \(N\) is the total number of documents in the collection and \(n(q)\) is the number of documents that contain the keyword \(q\).</p>
<p>Here is how we can implement this in Python:</p>
<pre><code class="language-python">import math

def get_idf(keyword, documents):
    N = len(documents)
    n_q = sum(1 for doc in documents if keyword in doc)

    idf = math.log((N - n_q + 0.5) / (n_q + 0.5) + 1)
    return idf

idf_i = get_idf("i", documents)
print(idf_i) # 0.24...

idf_password = get_idf("password", documents)
print(idf_password) # 0.69...

idf_ts01 = get_idf("TS-01", documents)
print(idf_ts01) # 1.54...
</code></pre>
<p>We can see that rare words like "TS-01" or "password" have a higher IDF score than more common words like "I".</p>
<p>Technically, the IDF score measures specificity, not importance.
After all, just because a word is rare doesn't necessarily mean it's important.
However, IDF is often used in algorithms that estimate word importance, because it increases the weight of rare terms—often desirable when building search engines.</p>
<p>Now, we can turn to the main topic of this section: exact match search, which we will implement using the <strong>BM25</strong> algorithm.</p>
<p>The core idea of BM25 is that the relevance of a document to a query depends on the frequency of the query terms in the document.</p>
<p>Consider a query \(q\) containing the keywords \(q_1, q_2, \ldots, q_n\) and a document \(d\).
The score of the document \(d\) for the query \(q\) is given by:</p>
<p>$$
\text{score}(q, d) = \sum_{i=1}^{n} \text{score}(q_i, d)
$$</p>
<p>How can we compute the score for a single keyword \(q_i\)?</p>
<p>We want the following properties from a good scoring function:</p>
<ol>
<li>Rare words matter more. We want the score to be proportional to the inverse document frequency of the keyword.</li>
<li>The more often the keyword appears in the document, the more relevant it is. We want the score to be proportional to the term frequency of the keyword in the document.</li>
<li>Longer documents should dilute relevance. We want the score to be inversely proportional to the document length.</li>
</ol>
<p>Here is what a first attempt at the score for a single keyword \(q_i\) might look like:</p>
<p>$$
\text{score}(q_i, d) = \text{IDF}(q_i) \cdot \frac{1}{1 + \frac{|d|}{f(q_i, d)}} = \text{IDF}(q_i) \cdot \frac{f(q_i, d)}{f(q_i, d) + |d|}
$$</p>
<p>where \(f(q_i, d)\) is the frequency of the keyword \(q_i\) in the document \(d\) and \(|d|\) is the length of the document.</p>
<p>It turns out that, in practice, this is not a good scoring function.
We need to stabilize it to avoid over-penalizing longer documents or over-rewarding repeated words.
A full derivation of the BM25 scoring function is beyond the scope of this book, so we will simply present the final formula:</p>
<p>$$
\text{score}(q_i, d) = \text{IDF}(q_i) \cdot \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}
$$</p>
<p>Here \(k_1\) and \(b\) are parameters that we can tune, typically setting them to \(k_1 \in [1.2, 2.0]\) and \(b \in [0.75, 1.0]\).
Additionally, \(avgdl\) is the average document length in the document collection.</p>
<p>Let's implement this in Python:</p>
<pre><code class="language-python">def get_bm25(query_keywords, document, documents, k1=1.5, b=0.75):
    avgdl = sum(len(doc) for doc in documents) / len(documents)

    doc_len = len(document)

    score = 0
    for keyword in query_keywords:
        f_qi_d = document.count(keyword)

        if f_qi_d == 0:
            continue

        idf = get_idf(keyword, documents)

        keyword_score = idf * (f_qi_d * (k1 + 1)) / (f_qi_d + k1 * (1 - b + b * (doc_len / avgdl)))
        score += keyword_score

    return score
</code></pre>
<p>Now we can test the implementation using an example query:</p>
<pre><code class="language-python">query = ["TS-01", "I", "password"]
for i, doc in enumerate(documents):
    score = get_bm25(query, doc, documents)
    print(f"Document {i+1} BM25 score: {round(score, 2)}")
</code></pre>
<p>This will output:</p>
<pre><code>Document 1 BM25 score: 2.53
Document 2 BM25 score: 0.84
Document 3 BM25 score: 0.33
Document 4 BM25 score: 0.31
Document 5 BM25 score: 1.01
Document 6 BM25 score: 0.34
</code></pre>
<p>The first document has by far the highest score which is exactly what we would expect thanks to the presence of the keyword "TS-01".
Note that it doesn't matter that the keyword "I" is absent from this document and present in the other documents because "I" is such a common word that its IDF is close to 0.
However, the presence of "TS-01" matters a great deal because it is a highly specific keyword and we value it accordingly.</p>
<p>In practice, we must convert every document and query into a list of keywords.
In this example, we simply split the documents and the query into individual words.
For real-world applications, however, we would use a more sophisticated method, such as removing stop words and applying stemming or lemmatization.</p>
<p>Now that we have an additional way to score documents by considering exact matches, we need to meaningfully combine the results of the semantic search and the keyword search.</p>
<p>Theoretically, we could take the union of the semantic search results and the keyword search results.
However, this approach would either return an excessively large set of documents or discard too many items.
Ranking the retrieved documents by relevance is therefore essential, and this is straightforward when working with a single search type.
For example, for the semantic search, we can use the cosine similarity to rank the documents, while for the keyword search, we can use the BM25 score.</p>
<p>But how can we rank documents that we have retrieved from two or more search types?
This is where <strong>rank fusion</strong> comes in.</p>
<p>The simplest rank fusion technique is <strong>reciprocal rank fusion</strong>.
For each retriever, we compute the reciprocal of the rank of the document plus a constant and sum the results.
The smaller the ranks for a document, the more important it is, and the higher the final score will be:</p>
<p>$$
\text{score}(d) = \sum_{r \in \text{retrievers}} \frac{1}{k + \text{rank}_r(d)}
$$</p>
<p>where \(k\) is a constant (typically \(k = 60\)) and \(\text{rank}_r(d)\) is the rank of the document \(d\) for the retriever \(r\).</p>
<p>Let's implement this in Python:</p>
<pre><code class="language-python">def rrf(first_results, second_results, k=60):
    all_docs = set(doc_id for doc_id, _ in first_results) | set(doc_id for doc_id, _ in second_results)

    first_ranks = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(first_results)}
    second_ranks = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(second_results)}

    rrf_scores = []
    for doc_id in all_docs:
        score = 0

        if doc_id in first_ranks:
            score += 1 / (k + first_ranks[doc_id])

        if doc_id in second_ranks:
            score += 1 / (k + second_ranks[doc_id])

        rrf_scores.append((doc_id, score))

    rrf_scores.sort(key=lambda x: x[1], reverse=True)
    return rrf_scores
</code></pre>
<p>Let's test this with an example:</p>
<pre><code class="language-python">semantic_results = [
    ("doc1", 0.95),
    ("doc3", 0.87),
    ("doc5", 0.82),
    ("doc2", 0.78),
    ("doc4", 0.65)
]

bm25_results = [
    ("doc2", 2.53),
    ("doc1", 1.84),
    ("doc4", 1.12),
    ("doc6", 0.95),
    ("doc3", 0.71)
]

fused_results = rrf(semantic_results, bm25_results)

for rank, (doc_id, score) in enumerate(fused_results, 1):
    print(f"{rank}. {doc_id}: {round(score, 4)}")
</code></pre>
<p>This will output:</p>
<pre><code>1. doc1: 0.0325
2. doc2: 0.032
3. doc3: 0.0315
4. doc4: 0.0313
5. doc5: 0.0159
6. doc6: 0.0156
</code></pre>
<p>We can see that the ranks of the documents depend on both the semantic search and the keyword search.</p>
<p>Rank fusion also works if you have more than two search types and is commonly used for balancing different search demands.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="03-generating-the-next-token.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="05-retrieval-augmented-generation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="03-generating-the-next-token.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="05-retrieval-augmented-generation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
