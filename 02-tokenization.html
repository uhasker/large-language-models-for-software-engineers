<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tokenization - Large Language Models for Software Engineers</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Large Language Models for Software Engineers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tokenization"><a class="header" href="#tokenization">Tokenization</a></h1>
<h2 id="llms-generate-text-by-generating-tokens"><a class="header" href="#llms-generate-text-by-generating-tokens">LLMs Generate Text by Generating Tokens</a></h2>
<p>LLMs take text as input and produce text as output.
However, they don't process text the way we do—not as characters or words.
Instead, they work with more general units called <strong>tokens</strong>.</p>
<p>A <strong>token</strong> is the basic unit of text that an LLM can process.
Whenever you feed text into an LLM, it's first split into tokens.
Tokens can be single characters, whole words, or even subword fragments.</p>
<p>For example, the sentence "Hello, world" might be split into the tokens <code>'Hello'</code>, <code>','</code>, <code>' '</code>, <code>'wo'</code>, <code>'rl'</code>, <code>'d'</code>.
The token <code>'Hello'</code> is a single word, the tokens <code>'wo'</code> and <code>'rl'</code> are subword fragments, and the tokens <code>','</code> and <code>' '</code> are single characters.
All of these are valid tokens.</p>
<p><img src="./images/tokens.svg" alt="Tokens" /></p>
<p>The set of all tokens available to an LLM is called the <strong>vocabulary</strong>.
Usually, the vocabulary of a modern LLM is very large, containing tens of thousands of tokens.</p>
<p>Now, here is the key point:
<strong>LLMs generate text one token at a time.</strong></p>
<p>When you feed text into an LLM, it's first split into tokens.
The model generates one token at a time, each based on the input plus all previously generated tokens.
This continues until it produces a special token called the <strong>end-of-sequence token</strong>, or reaches a predefined limit.</p>
<p>Consider the input text "How are you?".</p>
<p>The model first splits the text into tokens: <code>'How'</code>, <code>' '</code>, <code>'are'</code>, <code>' '</code>, <code>'you'</code>, <code>'?'</code>.</p>
<p>It then begins generating one token at a time.
The first token might be <code>'I'</code>, producing a new input text:
"How are you? I"</p>
<p>Next, it generates <code>' am'</code>, resulting in:
"How are you? I am"</p>
<p>Then comes <code>' fine'</code>:
"How are you? I am fine"</p>
<p>Followed by <code>'.'</code>:
"How are you? I am fine."</p>
<p>Finally, it may generate the special end-of-sequence token, signaling that it's done.</p>
<p>The final output text is "How are you? I am fine."
Usually, the end-of-sequence token is not included in the output text.</p>
<h2 id="the-tokenizer"><a class="header" href="#the-tokenizer">The Tokenizer</a></h2>
<p>The <strong>tokenizer</strong> is the LLM component that splits text into tokens and makes them digestible by the model.
Different LLMs use different tokenizers and not all of them split text the same way.
Tokenizers are usually trained on a large corpus of text and learn to split it in ways that are most useful for the model.</p>
<p>For example, the GPT models from OpenAI use a <strong>byte-pair encoding (BPE)</strong> tokenizer.
BPE starts with a vocabulary containing single characters and progressively merges the most frequent pairs of existing tokens to form new tokens.
The tokenizer might start with the vocabulary containing all the characters in the alphabet.
It might notice that 't' and 'h' often appear together and merge them into 'th'.
Later, it may merge 'th' and 'e' into 'the'.
This continues until a certain number of tokens is reached.</p>
<p>We can use the <code>tiktoken</code> library to see the tokens that a given LLM uses.</p>
<blockquote>
<p>If you don't want to go through the hassle of installing the library, you can also go to https://platform.openai.com/tokenizer and paste your text there.</p>
</blockquote>
<p>First, we need to install the library:</p>
<pre><code class="language-bash">pip install tiktoken
</code></pre>
<p>Then, we can use the library to get the tokens for a given model:</p>
<pre><code class="language-python">import tiktoken

# Get the tokenizer for the GPT-4o model
enc = tiktoken.encoding_for_model("gpt-4o")

# Encode a string into tokens
tokens = enc.encode("Hello, world")
print(tokens) # [13225, 11, 2375]
</code></pre>
<p>Interestingly enough, the tokens printed by the <code>tiktoken</code> library are integers, not strings.
This is because LLMs are neural networks that operate on numbers instead of text.
Therefore, the tokenizer not only splits the text into tokens, but also assigns a unique integer to each token called a <strong>token ID</strong>.</p>
<p>For example, the <code>gpt-4o</code> tokenizer assigns the token ID 13225 to the token <code>'Hello'</code>, 11 to the token <code>','</code>, and 2375 to the token <code>' world'</code>.</p>
<p>We can decode each token back into a string using the <code>decode_single_token_bytes</code> method:</p>
<pre><code class="language-python">decoded_text = enc.decode_single_token_bytes(13225)
print(decoded_text) # b'Hello'

decoded_text = enc.decode_single_token_bytes(11)
print(decoded_text) # b','

decoded_text = enc.decode_single_token_bytes(2375)
print(decoded_text) # b' world'
</code></pre>
<p>Notice that the last token isn't <code>'world'</code>, but <code>' world'</code>— with a leading space.
In fact, there are two different tokens for <code>'world'</code> and <code>' world'</code>:</p>
<pre><code class="language-python">print(enc.encode("world")) # [24169]
print(enc.encode(" world")) # [2375]
</code></pre>
<p>This results from how BPE works: because it frequently merges tokens that appear together, and words often follow a space, most words end up with two versions—one with a leading space and one without.
This helps reduce token usage and improves the model's understanding of word boundaries.</p>
<p>Instead of decoding tokens one by one using the <code>decode_single_token_bytes</code> method, we can also decode the entire list of tokens at once using the <code>decode</code> method:</p>
<pre><code class="language-python">decoded_text = enc.decode(tokens)
print(decoded_text) # 'Hello, world'
</code></pre>
<blockquote>
<p>Warning: While you can apply <code>decode</code> to single tokens, doing so may be lossy if the token does not align with UTF-8 character boundaries.</p>
</blockquote>
<p>Remember how we said that the tokens are often subwords instead of whole words?
Here is an example of a tokenization where a single word is split into multiple tokens:</p>
<pre><code class="language-python">tokens = enc.encode("Deoxyribonucleic acid")
print(tokens) # [1923, 1233, 3866, 53047, 68714, 291, 18655]
</code></pre>
<p>Let's print the tokens one by one:</p>
<pre><code class="language-python">for token in tokens:
    print(enc.decode_single_token_bytes(token))
</code></pre>
<p>This will output:</p>
<pre><code>b'De'
b'ox'
b'yr'
b'ibon'
b'ucle'
b'ic'
b' acid'
</code></pre>
<p>We can see that the word "Deoxyribonucleic" is split into 6 tokens.</p>
<h2 id="token-pricing"><a class="header" href="#token-pricing">Token Pricing</a></h2>
<p>Understanding tokens is key to understanding how LLMs are priced because model providers typically charge per token.
For example, the <a href="https://platform.openai.com/docs/pricing">OpenAI Pricing page</a> lists prices per million tokens—not per request.</p>
<p>Let's say you want to use the <code>gpt-4o</code> model in your application and you want to estimate the cost of a given prompt.
The OpenAI pricing page gives two prices—one price for input tokens and one price for output tokens.
In this particular instance, they charge $2.50 per million input tokens and $10 per million output tokens.</p>
<blockquote>
<p>Output tokens are more expensive than input tokens because they have to be generated by the model one by one.</p>
</blockquote>
<p>Therefore, if you had a prompt containing 1000 tokens that is expected to generate 2000 tokens, the cost for the input tokens would be $2.50 * 1000 / 1000000 = $0.0025 and the cost for the output tokens would be $10 * 2000 / 1000000 = $0.02.</p>
<p>The total cost for the prompt would be $0.0025 + $0.02 = $0.0225.</p>
<p>This is a very simple example, but it already illustrates that your costs will be mostly determined by the number of output tokens that you generate.</p>
<p>That makes practical cost estimation a bit tricky because the number of output tokens is typically not known in advance.
Nevertheless, you can produce reasonable estimates by sending a few example requests and averaging the results.
You can either do this manually by counting the output tokens using the <code>tiktoken</code> library or by inspecting the response using the OpenAI API which gives you a <code>usage</code> object containing the number of input and output tokens:</p>
<pre><code class="language-python">import os, requests

response = requests.post(
    "https://api.openai.com/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
        "Content-Type": "application/json",
    },
    json={
        "model": "gpt-4o",
        "messages": [
            {"role": "user", "content": "Hello, world"}
        ]
    }
)

response_json = response.json()
usage = response_json["usage"]
prompt_tokens = usage["prompt_tokens"]
completion_tokens = usage["completion_tokens"]
total_tokens = usage["total_tokens"]

print(f"Completion: {response_json['choices'][0]['message']['content']}")

print(f"Prompt tokens: {prompt_tokens}")
print(f"Completion tokens: {completion_tokens}")
print(f"Total tokens: {total_tokens}")
</code></pre>
<p>This will output something along the lines of:</p>
<pre><code class="language-python">Completion: Hello! How can I assist you today?

Prompt tokens: 10
Completion tokens: 9
Total tokens: 19
</code></pre>
<p>OpenAI includes formatting tokens for message roles in the total token count—which is why you see 10 prompt tokens instead of just 4.
For most practical purposes, this difference will be negligible.</p>
<p>Once you've estimated how many tokens your application will use, it's a good idea to set a maximum token limit.
A common approach is to allow about 50% more than your estimate, using the <code>max_tokens</code> or <code>max_completion_tokens</code> parameter.
This helps avoid unexpected costs while giving you a bit of buffer.</p>
<h2 id="the-context-window"><a class="header" href="#the-context-window">The Context Window</a></h2>
<p>Every large language model has a <strong>context window</strong>—the maximum number of tokens that can be processed in a single request.
For example, the <code>gpt-4o</code> model has a context window of 128k tokens.</p>
<p>If you try to process more tokens than the context window allows, most APIs will throw an error while most chat interfaces will silently truncate the request.
This is especially important to keep in mind when you have long multi-turn conversations or are processing large files.</p>
<p>To avoid overflowing the context window, you can summarize or compress earlier content, truncate low-priority sections, use sliding windows or chunk long content into smaller pieces.
We will discuss some of these approaches in later chapters.</p>
<p>For now, it's important to understand that the context window is a hard limit on what you can process and that you need to keep that in mind when you design your application.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="01-text-in-text-out.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="03-generating-the-next-token.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="01-text-in-text-out.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="03-generating-the-next-token.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
